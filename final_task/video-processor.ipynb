{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom typing import Tuple, Optional\n\nclass VideoProcessor:\n    def __init__(self, model, device, target_size: Tuple[int, int] = (256, 256)):\n        self.model = model\n        self.device = device\n        self.target_size = target_size\n        self.model.eval()\n\n    def preprocess_frame(self, frame: np.ndarray, transform=None) -> torch.Tensor:\n\n        frame_resized = cv2.resize(frame, self.target_size)\n        \n        frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n        \n        if transform:\n            try:\n                augmented = transform(image=frame_rgb)\n                frame_tensor = augmented['image'].unsqueeze(0).to(self.device)\n            except Exception as e:\n                print(f\"Transform error: {e}\")\n                # Fallback preprocessing\n                frame_tensor = self._basic_preprocessing(frame_rgb)\n        else:\n            frame_tensor = self._basic_preprocessing(frame_rgb)\n            \n        return frame_tensor\n\n    def _basic_preprocessing(self, frame_rgb: np.ndarray) -> torch.Tensor:\n\n        frame_tensor = torch.from_numpy(frame_rgb.transpose(2, 0, 1)).float()\n        frame_tensor = frame_tensor.unsqueeze(0).to(self.device)\n        frame_tensor = frame_tensor / 255.0\n        return frame_tensor\n\n    def process_frame(self, frame: np.ndarray, transform=None) -> Tuple[np.ndarray, np.ndarray]:\n        original_size = frame.shape[:2][::-1]  # (width, height)\n        \n        frame_tensor = self.preprocess_frame(frame, transform)\n                with torch.no_grad():\n            try:\n                output = self.model(frame_tensor)\n                pred_mask = torch.argmax(output, dim=1).cpu().numpy()[0]\n                \n                # Resize mask back to original size\n                pred_mask_resized = cv2.resize(\n                    pred_mask.astype(np.float32), \n                    original_size,\n                    interpolation=cv2.INTER_NEAREST\n                ).astype(np.int32)\n                \n                return pred_mask, pred_mask_resized\n            except Exception as e:\n                print(f\"Prediction error: {e}\")\n                return None, None\n\n    def create_colored_mask(self, pred_mask: np.ndarray) -> np.ndarray:\n\n        height, width = pred_mask.shape\n        colored_mask = np.zeros((height, width, 3), dtype=np.uint8)\n        \n        for class_id, (name, color) in IDD_CLASSES.items():\n            colored_mask[pred_mask == class_id] = color\n        \n        return colored_mask\n\n    def add_labels_to_mask(self, colored_mask: np.ndarray, pred_mask: np.ndarray) -> np.ndarray:\n        mask_with_labels = colored_mask.copy()\n        height, width = pred_mask.shape\n        \n        font_scale = min(width, height) / 1000.0\n        min_font_scale = 0.3\n        font_scale = max(font_scale, min_font_scale)\n        \n        unique_classes = np.unique(pred_mask)\n        \n        for class_id in unique_classes:\n            if class_id == 13:  \n                continue\n                \n            class_name = IDD_CLASSES[class_id][0]\n            \n            y_coords, x_coords = np.where(pred_mask == class_id)\n            if len(y_coords) > 0:\n                mask = (pred_mask == class_id).astype(np.uint8)\n                num_labels, labels = cv2.connectedComponents(mask)\n                \n                for label in range(1, num_labels):\n                    component_mask = (labels == label)\n                    if np.sum(component_mask) > 100: \\\n                        y_comp, x_comp = np.where(component_mask)\n                        center_y = int(np.mean(y_comp))\n                        center_x = int(np.mean(x_comp))\n                        \n                        cv2.putText(\n                            mask_with_labels, \n                            class_name,\n                            (center_x, center_y),\n                            cv2.FONT_HERSHEY_SIMPLEX,\n                            font_scale,\n                            (255, 255, 255),\n                            max(1, int(font_scale * 2)),\n                            cv2.LINE_AA\n                        )\n        \n        return mask_with_labels\n\n    def process_video(self, video_path: str, output_path: str, transform=None, overlay: bool = False):\n        try:\n            cap = cv2.VideoCapture(video_path)\n            if not cap.isOpened():\n                raise ValueError(f\"Could not open video file: {video_path}\")\n            \n            frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n            frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n            fps = int(cap.get(cv2.CAP_PROP_FPS))\n            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n            \n            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n            out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n            \n            with tqdm(total=total_frames, desc=\"Processing video\") as pbar:\n                frame_count = 0\n                while cap.isOpened():\n                    ret, frame = cap.read()\n                    if not ret:\n                        break\n                        \n                    # Process frame\n                    _, pred_mask_resized = self.process_frame(frame, transform)\n                    if pred_mask_resized is not None:\n                        colored_mask = self.create_colored_mask(pred_mask_resized)\n                        labeled_mask = self.add_labels_to_mask(colored_mask, pred_mask_resized)\n                        \n                        if overlay:\n                            # Overlay mask on original frame\n                            alpha = 0.5\n                            output_frame = cv2.addWeighted(frame, 1-alpha, labeled_mask, alpha, 0)\n                        else:\n                            output_frame = labeled_mask\n                            \n                        out.write(output_frame)\n                    \n                    frame_count += 1\n                    pbar.update(1)\n            \n            print(f\"Processed {frame_count} frames successfully\")\n            \n        except Exception as e:\n            print(f\"Error processing video: {e}\")\n        \n        finally:\n            if 'cap' in locals():\n                cap.release()\n            if 'out' in locals():\n                out.release()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmodel = UNet(n_channels=3, n_classes=14).to(device)\nmodel.load_state_dict(torch.load('best_model.pth'))\n\nprocessor = VideoProcessor(\n    model=model,\n    device=device,\n    target_size=(256, 256)  # Model's target size\n)\n\nprocessor.process_video(\n    video_path=\"any_size_video.mp4\",\n    output_path=\"processed_output.mp4\",\n    transform=get_transforms(\"val\"),\n    overlay=True  # Set to False for mask-only output\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}